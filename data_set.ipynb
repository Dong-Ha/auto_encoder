{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "normal = pd.read_csv(\"C:/Users/tlseh/Desktop/WADI_14days_new.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "normal_drop = normal.drop(['Row','Date','Time'],axis='columns')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "normal_drop_1 = normal_drop[0:1000] # 앞에 1000개만 우선 해보기 위해서 부분 선택\r\n",
    "normal_drop_1 = normal_drop_1.dropna(axis=1) # 값이 없는 열 제거 여기서는 행 4개가 제거되었다."
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "normal_drop_1"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     1_AIT_001_PV  1_AIT_002_PV  1_AIT_003_PV  1_AIT_004_PV  1_AIT_005_PV  \\\n",
       "0         171.155      0.619473       11.5759       504.645      0.318319   \n",
       "1         171.155      0.619473       11.5759       504.645      0.318319   \n",
       "2         171.155      0.619473       11.5759       504.645      0.318319   \n",
       "3         171.155      0.607477       11.5725       504.673      0.318438   \n",
       "4         171.155      0.607477       11.5725       504.673      0.318438   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "995       174.938      0.559481       11.7524       484.411      0.286654   \n",
       "996       174.938      0.559481       11.7524       484.411      0.286654   \n",
       "997       174.938      0.559481       11.7524       484.411      0.286654   \n",
       "998       174.213      0.541483       11.7854       482.401      0.286422   \n",
       "999       174.213      0.541483       11.7854       482.401      0.286422   \n",
       "\n",
       "     1_FIT_001_PV  1_LS_001_AL  1_LS_002_AL  1_LT_001_PV  1_MV_001_STATUS  \\\n",
       "0        0.001157            0            0      47.8911                1   \n",
       "1        0.001157            0            0      47.8911                1   \n",
       "2        0.001157            0            0      47.8911                1   \n",
       "3        0.001207            0            0      47.7503                1   \n",
       "4        0.001207            0            0      47.7503                1   \n",
       "..            ...          ...          ...          ...              ...   \n",
       "995      1.907730            0            0      40.5656                2   \n",
       "996      1.907730            0            0      40.5656                2   \n",
       "997      1.907730            0            0      40.5656                2   \n",
       "998      2.003600            0            0      40.8926                2   \n",
       "999      2.003600            0            0      40.8926                2   \n",
       "\n",
       "     ...  3_MV_001_STATUS  3_MV_002_STATUS  3_MV_003_STATUS  3_P_001_STATUS  \\\n",
       "0    ...                1                1                1               1   \n",
       "1    ...                1                1                1               1   \n",
       "2    ...                1                1                1               1   \n",
       "3    ...                1                1                1               1   \n",
       "4    ...                1                1                1               1   \n",
       "..   ...              ...              ...              ...             ...   \n",
       "995  ...                1                1                1               1   \n",
       "996  ...                1                1                1               1   \n",
       "997  ...                1                1                1               1   \n",
       "998  ...                1                1                1               1   \n",
       "999  ...                1                1                1               1   \n",
       "\n",
       "     3_P_002_STATUS  3_P_003_STATUS  3_P_004_STATUS  LEAK_DIFF_PRESSURE  \\\n",
       "0                 1               1               1             67.9651   \n",
       "1                 1               1               1             67.9651   \n",
       "2                 1               1               1             67.9651   \n",
       "3                 1               1               1             67.1948   \n",
       "4                 1               1               1             67.1948   \n",
       "..              ...             ...             ...                 ...   \n",
       "995               1               1               1             63.8467   \n",
       "996               1               1               1             63.8467   \n",
       "997               1               1               1             63.8467   \n",
       "998               1               1               1             63.9204   \n",
       "999               1               1               1             63.9204   \n",
       "\n",
       "     PLANT_START_STOP_LOG  TOTAL_CONS_REQUIRED_FLOW  \n",
       "0                       1                      0.68  \n",
       "1                       1                      0.68  \n",
       "2                       1                      0.68  \n",
       "3                       1                      0.68  \n",
       "4                       1                      0.68  \n",
       "..                    ...                       ...  \n",
       "995                     1                      0.68  \n",
       "996                     1                      0.68  \n",
       "997                     1                      0.68  \n",
       "998                     1                      0.68  \n",
       "999                     1                      0.68  \n",
       "\n",
       "[1000 rows x 123 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1_AIT_001_PV</th>\n",
       "      <th>1_AIT_002_PV</th>\n",
       "      <th>1_AIT_003_PV</th>\n",
       "      <th>1_AIT_004_PV</th>\n",
       "      <th>1_AIT_005_PV</th>\n",
       "      <th>1_FIT_001_PV</th>\n",
       "      <th>1_LS_001_AL</th>\n",
       "      <th>1_LS_002_AL</th>\n",
       "      <th>1_LT_001_PV</th>\n",
       "      <th>1_MV_001_STATUS</th>\n",
       "      <th>...</th>\n",
       "      <th>3_MV_001_STATUS</th>\n",
       "      <th>3_MV_002_STATUS</th>\n",
       "      <th>3_MV_003_STATUS</th>\n",
       "      <th>3_P_001_STATUS</th>\n",
       "      <th>3_P_002_STATUS</th>\n",
       "      <th>3_P_003_STATUS</th>\n",
       "      <th>3_P_004_STATUS</th>\n",
       "      <th>LEAK_DIFF_PRESSURE</th>\n",
       "      <th>PLANT_START_STOP_LOG</th>\n",
       "      <th>TOTAL_CONS_REQUIRED_FLOW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>171.155</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>11.5759</td>\n",
       "      <td>504.645</td>\n",
       "      <td>0.318319</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.8911</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.9651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>171.155</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>11.5759</td>\n",
       "      <td>504.645</td>\n",
       "      <td>0.318319</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.8911</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.9651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>171.155</td>\n",
       "      <td>0.619473</td>\n",
       "      <td>11.5759</td>\n",
       "      <td>504.645</td>\n",
       "      <td>0.318319</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.8911</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.9651</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.155</td>\n",
       "      <td>0.607477</td>\n",
       "      <td>11.5725</td>\n",
       "      <td>504.673</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.7503</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.1948</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>171.155</td>\n",
       "      <td>0.607477</td>\n",
       "      <td>11.5725</td>\n",
       "      <td>504.673</td>\n",
       "      <td>0.318438</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>47.7503</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>67.1948</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>174.938</td>\n",
       "      <td>0.559481</td>\n",
       "      <td>11.7524</td>\n",
       "      <td>484.411</td>\n",
       "      <td>0.286654</td>\n",
       "      <td>1.907730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.5656</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.8467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>174.938</td>\n",
       "      <td>0.559481</td>\n",
       "      <td>11.7524</td>\n",
       "      <td>484.411</td>\n",
       "      <td>0.286654</td>\n",
       "      <td>1.907730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.5656</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.8467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>174.938</td>\n",
       "      <td>0.559481</td>\n",
       "      <td>11.7524</td>\n",
       "      <td>484.411</td>\n",
       "      <td>0.286654</td>\n",
       "      <td>1.907730</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.5656</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.8467</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>174.213</td>\n",
       "      <td>0.541483</td>\n",
       "      <td>11.7854</td>\n",
       "      <td>482.401</td>\n",
       "      <td>0.286422</td>\n",
       "      <td>2.003600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.8926</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.9204</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>174.213</td>\n",
       "      <td>0.541483</td>\n",
       "      <td>11.7854</td>\n",
       "      <td>482.401</td>\n",
       "      <td>0.286422</td>\n",
       "      <td>2.003600</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40.8926</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63.9204</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 123 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "normal_drop_1.isnull().sum().sum() # 성분에서 NaN이 있으면 1이상의 값이 나오게 된다. pd.DataFrame.isnull().sum().sum()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "from sklearn import preprocessing       # 정규화를 반드시 진행해야 하는 건지?\r\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\r\n",
    "\r\n",
    "x = normal_drop_1.values\r\n",
    "x_scaled = min_max_scaler.fit_transform(x)\r\n",
    "normal_drop_1_scaled = pd.DataFrame(x_scaled)\r\n",
    "\r\n",
    "print(normal_drop_1_scaled)\r\n",
    "\r\n",
    "normal_drop_1 = normal_drop_1_scaled   # 정규화 된 데이터를 넣어주어야함! sklearn 사용하면 정규화 복구하는데도 편리하다.\r\n",
    "\r\n",
    "normal_drop_1 = np.array(normal_drop_1) # Dataframe을 numpy로 변환"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          0         1         2         3         4         5    6    7    \\\n",
      "0    0.006593  0.529401  0.252453  0.919934  0.824437  0.000094  0.0  0.0   \n",
      "1    0.006593  0.529401  0.252453  0.919934  0.824437  0.000094  0.0  0.0   \n",
      "2    0.006593  0.529401  0.252453  0.919934  0.824437  0.000094  0.0  0.0   \n",
      "3    0.006593  0.470590  0.240948  0.921092  0.827501  0.000118  0.0  0.0   \n",
      "4    0.006593  0.470590  0.240948  0.921092  0.827501  0.000118  0.0  0.0   \n",
      "..        ...       ...       ...       ...       ...       ...  ...  ...   \n",
      "995  0.662908  0.235287  0.849746  0.083127  0.009064  0.923259  0.0  0.0   \n",
      "996  0.662908  0.235287  0.849746  0.083127  0.009064  0.923259  0.0  0.0   \n",
      "997  0.662908  0.235287  0.849746  0.083127  0.009064  0.923259  0.0  0.0   \n",
      "998  0.537127  0.147052  0.961421  0.000000  0.003090  0.969679  0.0  0.0   \n",
      "999  0.537127  0.147052  0.961421  0.000000  0.003090  0.969679  0.0  0.0   \n",
      "\n",
      "          8    9    ...  113  114  115  116  117  118  119       120  121  122  \n",
      "0    0.779419  0.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.102249  0.0  0.0  \n",
      "1    0.779419  0.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.102249  0.0  0.0  \n",
      "2    0.779419  0.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.102249  0.0  0.0  \n",
      "3    0.765998  0.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.092489  0.0  0.0  \n",
      "4    0.765998  0.5  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.092489  0.0  0.0  \n",
      "..        ...  ...  ...  ...  ...  ...  ...  ...  ...  ...       ...  ...  ...  \n",
      "995  0.081121  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.050069  0.0  0.0  \n",
      "996  0.081121  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.050069  0.0  0.0  \n",
      "997  0.081121  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.050069  0.0  0.0  \n",
      "998  0.112292  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.051003  0.0  0.0  \n",
      "999  0.112292  1.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.051003  0.0  0.0  \n",
      "\n",
      "[1000 rows x 123 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#  내가 입력차원을 잘못 생각함, 반대로 해주어야한다.\r\n",
    "\r\n",
    "# normal_drop_1.shape\r\n",
    "# print(\"before transpose :{}\".format(normal_drop_1.shape)) # 데이터셋 입력전 rnn 모양을 맞춰주기 위해서 행열 transpose\r\n",
    "# normal_drop_1 = np.transpose(normal_drop_1)\r\n",
    "# print(\"after transpose :{}\".format(normal_drop_1.shape))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "normal_drop_1 = torch.from_numpy(normal_drop_1).unsqueeze(0) # 배치 차원을 추가 해주기 위해서 첫번째 위치에 차원 추가"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "normal_drop_1.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 1000, 123])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def Window(data,window,step):   # window 구현을 어떻게 해야 할지 몰라서 함수로 구현\r\n",
    "    batch_size = (data.shape[1] - window) // step # 두번째 인덱스가 전체 샘플 개수\r\n",
    "    Window_complete = torch.tensor([])\r\n",
    "\r\n",
    "    for index in range(batch_size):\r\n",
    "        x = torch.clone(data[:, step * index : window + step * index,:])\r\n",
    "        Window_complete = torch.cat((Window_complete,x),dim=0) \r\n",
    "\r\n",
    "    return Window_complete # (batch,window,sensor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "window=5\r\n",
    "step=3\r\n",
    "data = Window(normal_drop_1,window=window,step=step)\r\n",
    "print(data.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([331, 5, 123])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from torch.utils.data import Dataset\r\n",
    "from torch.utils.data import DataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "class CustomDataset(Dataset):   # 데이터셋에 굳이 넣어야 하는건지?\r\n",
    "  def __init__(self,data):\r\n",
    "      self.data = data\r\n",
    "\r\n",
    "  def __getitem__(self, idx): \r\n",
    "    return self.data[idx]\r\n",
    "\r\n",
    "  def __len__(self): \r\n",
    "    return len(self.data)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "dataset = CustomDataset(data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "dataset.data.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([331, 5, 123])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "SEQ_LEN = dataset.data.shape[1]\r\n",
    "INPUT_SIZE = dataset.data.shape[2]\r\n",
    "BATCH_SIZE = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dataloader = DataLoader(dataset,batch_size=BATCH_SIZE, shuffle=False,drop_last=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "from torch import nn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class Encoder(nn.Module):\r\n",
    "    def __init__(self, input_size, embedding_dim=32):\r\n",
    "        super(Encoder, self).__init__()\r\n",
    "\r\n",
    "        self.input_size = input_size\r\n",
    "        self.hidden_dim = embedding_dim * 2\r\n",
    "        self.embedding_dim = embedding_dim \r\n",
    "\r\n",
    "        \r\n",
    "        self.rnn1 = nn.LSTM(input_size=self.input_size,hidden_size=self.hidden_dim,num_layers=1,batch_first=True)\r\n",
    "        self.rnn2 = nn.LSTM(input_size=self.hidden_dim,hidden_size=self.embedding_dim,num_layers=1,batch_first=True)\r\n",
    "\r\n",
    "    def forward(self, encoder_output):           # (batch size, window, sensor=123)\r\n",
    "        encoder_output = encoder_output.float()\r\n",
    "        encoder_output,(_,_) = self.rnn1(encoder_output)\r\n",
    "        encoder_output, (hidden,_) = self.rnn2(encoder_output) # encoder_output = (batch size, window, embedding_dim)\r\n",
    "\r\n",
    "        return encoder_output, hidden    # hidden = (num_layers=1, batch size, embedding_dim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "class Attention(nn.Module):\r\n",
    "    def __init__(self,enc_embedding_dim, dec_embedding_dim):\r\n",
    "        super(Attention, self).__init__()\r\n",
    "        self.attn = nn.Linear(enc_embedding_dim *2,dec_embedding_dim)\r\n",
    "        self.v = nn.Linear(dec_embedding_dim, 1, bias = False)\r\n",
    "        \r\n",
    "\r\n",
    "    def forward(self,encoder_output, hidden):   # encoder_output = (batch size, window, embedding_dim)\r\n",
    "                                                # hidden = (num_layers=1, batch size, embedding_dim)\r\n",
    "\r\n",
    "        hidden = hidden.premute(1,0,2).repeat(1,window,1)  # hidden = (batch size, window, embedding_dim)\r\n",
    "        \r\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_output),dim = 2)))\r\n",
    "        # enegry = (batch size, window, dec_embedding_dim)\r\n",
    "\r\n",
    "        attention = self.v(energy) # attention = (batch size, window, 1)\r\n",
    "        attention = F.softmax(attention, dim=1)\r\n",
    "        \r\n",
    "        attention = torch.mul(attention,encoder_output)\r\n",
    "        return  attention   # attention = (batch size, window, embedding_dim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "class Decoder(nn.Module):\r\n",
    "    def __init__(self,input_size, output_size, attention):\r\n",
    "        super(Decoder, self).__init__()\r\n",
    "\r\n",
    "        self.input_size = input_size\r\n",
    "        self.hidden_dim = input_size * 2\r\n",
    "        self.output_size = output_size\r\n",
    "\r\n",
    "        self.attention = attention\r\n",
    "        self.rnn1 = nn.LSTM(input_size=self.input_size,hidden_size=self.hidden_dim,batch_first=True)\r\n",
    "        self.rnn2 = nn.LSTM(input_size=self.hidden_dim,hidden_size=self.output_size,batch_first=True)\r\n",
    "\r\n",
    "    def forward(self,input, hidden, encoder_outputs):# hidden = (num_layers=1, batch size, embedding_dim)\r\n",
    "        hidden = hidden.permute(1,0,2)    # hidden = (batch size, num_layers=1, embedding_dim)\r\n",
    "        hidden = hidden.repeat(1,window,1)# hidden = (batch size, window, embedding_dim)\r\n",
    "        attention = self.attention(encoder_outputs, hidden)\r\n",
    "        x = hidden + attention\r\n",
    "        x,(_,_) = self.rnn1(x)\r\n",
    "        x,(_,_) = self.rnn2(x)  # (batch_size,window,output_size)\r\n",
    "        return x "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "class Autoencoder(nn.Module):\r\n",
    "\r\n",
    "  def __init__(self,INPUT_SIZE, EMBEDDING_DIM=32):\r\n",
    "    super(Autoencoder, self).__init__()\r\n",
    "\r\n",
    "    self.encoder = Encoder(input_size=INPUT_SIZE, embedding_dim=EMBEDDING_DIM)\r\n",
    "    self.decoder = Decoder(input_size=EMBEDDING_DIM, output_size=INPUT_SIZE)\r\n",
    "\r\n",
    "  def forward(self, x):\r\n",
    "    encoder_output, hidden = self.encoder(x)\r\n",
    "    x = self.decoder(x)\r\n",
    "\r\n",
    "    return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "model = Autoencoder(INPUT_SIZE,32)\r\n",
    "optimizer = torch.optim.Adam(model.parameters())\r\n",
    "#criterion = nn.CrossEntropyLoss()\r\n",
    "criterion = nn.L1Loss(reduction='sum')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "#for i in range(100):\r\n",
    "for i, batch in enumerate(dataloader):\r\n",
    "        \r\n",
    "    optimizer.zero_grad()\r\n",
    "    \r\n",
    "    output = model(batch)\r\n",
    "    \r\n",
    "    loss = criterion(output, batch)\r\n",
    "    \r\n",
    "    loss.backward()\r\n",
    "\r\n",
    "    if i % 10 == 0:\r\n",
    "        print(\"NOW index : {}, LOSS : {}\".format(i, loss))\r\n",
    "\r\n",
    "    optimizer.step()\r\n",
    "    "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "NOW index : 0, LOSS : 369.7671813964844\n",
      "NOW index : 10, LOSS : 364.8744812011719\n",
      "NOW index : 20, LOSS : 411.13262939453125\n",
      "NOW index : 30, LOSS : 258.9564208984375\n",
      "NOW index : 40, LOSS : 163.02294921875\n",
      "NOW index : 50, LOSS : 100.57623291015625\n",
      "NOW index : 60, LOSS : 82.93132019042969\n",
      "NOW index : 70, LOSS : 418.251708984375\n",
      "NOW index : 80, LOSS : 216.02322387695312\n",
      "NOW index : 90, LOSS : 114.42770385742188\n",
      "NOW index : 100, LOSS : 95.28270721435547\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('lab': conda)"
  },
  "interpreter": {
   "hash": "d0bda0206111b919826ea007f7bf17d966c84c6481f60aa426c1e7f2ea1ecded"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}